{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterSentiAnalysis\")\n",
    "conf.set(\"spark.network.timeout\",\"100000s\")\n",
    "sc = SparkContext(conf=conf)\n",
    "   \n",
    "spark = SparkSession.builder.appName(\"TwitterSentiAnalysis\").getOrCreate()\n",
    "#to start a spark context\n",
    "#read data file\n",
    "data=sc.textFile(\"D:/HPC-proj/datasetfinal.csv\")\n",
    "head=data.first()\n",
    "\n",
    "pos = sc.textFile(\"D:/HPC-proj/pos1.txt\")\n",
    "neg = sc.textFile(\"D:/HPC-proj/neg1.txt\")\n",
    "pos_sp = pos.flatMap(lambda line: line.split(\"\\n\")).collect()\n",
    "neg_sp = neg.flatMap(lambda line: line.split(\"\\n\")).collect()\n",
    "all_words = []\n",
    "documents = []\n",
    "allowed = [\"J\", \"R\", \"V\", \"N\"]\n",
    "for p in pos_sp:\n",
    "    documents.append({\"text\": p , \"label\": 1})\n",
    "\n",
    "for p in neg_sp:\n",
    "    documents.append({\"text\": p , \"label\": 0})\n",
    "\n",
    "def wc(data):\n",
    "    words = word_tokenize(data)\n",
    "    tag = nltk.pos_tag(words)\n",
    "    for w in tag:\n",
    "        if w[1][0] in allowed:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "    return all_words\n",
    "\n",
    "\n",
    "raw_data = sc.parallelize(documents, numSlices=100)\n",
    "raw_tokenized = raw_data.map(lambda dic : {\"text\": wc(dic[\"text\"]) , \"label\" : dic[\"label\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "htf = HashingTF(50000)\n",
    "raw_hashed = raw_tokenized.map(lambda dic : LabeledPoint(dic[\"label\"], htf.transform(dic[\"text\"])))\n",
    "raw_hashed.persist()\n",
    "trained_hashed, test_hashed = raw_hashed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(row):\n",
    "    split_row=row.split(\",\")\n",
    "    return (split_row[0],split_row[1].split(\" \"))\n",
    "\n",
    "data=data.filter(lambda r: r!=head)\n",
    "dataset=data.map(lambda r: s(r))\n",
    "#convert rdd into sql dataframe to remove stop words, make ngram model and turn review into a feature\n",
    "dataframe=spark.createDataFrame(dataset, [\"sentiment\", \"tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|sentiment|               tweet|\n",
      "+---------+--------------------+\n",
      "|        0|[is, so, sad, for...|\n",
      "|        0|[i, missed, the, ...|\n",
      "|        1|[omg, its, alread...|\n",
      "|        0|[omgaga, im, sooo...|\n",
      "|        0|[i, think, mi, bf...|\n",
      "|        0|[or, i, just, wor...|\n",
      "|        1|[juuuuuuuuuuuuuuu...|\n",
      "|        0|[sunny, again, wo...|\n",
      "|        1|[handed, in, my, ...|\n",
      "|        1|[hmmmm, i, wonder...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover=StopWordsRemover(inputCol=\"tweet\", outputCol=\"filtered\")\n",
    "filtered_df=remover.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|sentiment|               tweet|            filtered|\n",
      "+---------+--------------------+--------------------+\n",
      "|        0|[is, so, sad, for...|  [sad, apl, friend]|\n",
      "|        0|[i, missed, the, ...|[missed, new, moo...|\n",
      "|        1|[omg, its, alread...|   [omg, already, o]|\n",
      "|        0|[omgaga, im, sooo...|[omgaga, im, sooo...|\n",
      "|        0|[i, think, mi, bf...|[think, mi, bf, c...|\n",
      "|        0|[or, i, just, wor...|       [worry, much]|\n",
      "|        1|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|\n",
      "|        0|[sunny, again, wo...|[sunny, work, tom...|\n",
      "|        1|[handed, in, my, ...|[handed, uniform,...|\n",
      "|        1|[hmmmm, i, wonder...|[hmmmm, wonder, n...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make 2-gram model\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "ngram=NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "gram_df=ngram.transform(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|sentiment|               tweet|            filtered|               2gram|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        0|[is, so, sad, for...|  [sad, apl, friend]|[sad apl, apl fri...|\n",
      "|        0|[i, missed, the, ...|[missed, new, moo...|[missed new, new ...|\n",
      "|        1|[omg, its, alread...|   [omg, already, o]|[omg already, alr...|\n",
      "|        0|[omgaga, im, sooo...|[omgaga, im, sooo...|[omgaga im, im so...|\n",
      "|        0|[i, think, mi, bf...|[think, mi, bf, c...|[think mi, mi bf,...|\n",
      "|        0|[or, i, just, wor...|       [worry, much]|        [worry much]|\n",
      "|        1|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|\n",
      "|        0|[sunny, again, wo...|[sunny, work, tom...|[sunny work, work...|\n",
      "|        1|[handed, in, my, ...|[handed, uniform,...|[handed uniform, ...|\n",
      "|        1|[hmmmm, i, wonder...|[hmmmm, wonder, n...|[hmmmm wonder, wo...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gram_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make term frequency vectors out of data frame to feed machine\n",
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "hashingtf=HashingTF(inputCol=\"2gram\", outputCol=\"tf\", numFeatures=20000)\n",
    "tf_df=hashingtf.transform(gram_df)\n",
    "#tf-idf\n",
    "idf=IDF(inputCol=\"tf\", outputCol=\"idftf\")\n",
    "idfModel=idf.fit(tf_df)\n",
    "idf_df=idfModel.transform(tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|               tweet|            filtered|               2gram|                  tf|               idftf|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        0|[is, so, sad, for...|  [sad, apl, friend]|[sad apl, apl fri...|(20000,[9509,1977...|(20000,[9509,1977...|\n",
      "|        0|[i, missed, the, ...|[missed, new, moo...|[missed new, new ...|(20000,[1358,2474...|(20000,[1358,2474...|\n",
      "|        1|[omg, its, alread...|   [omg, already, o]|[omg already, alr...|(20000,[11657,197...|(20000,[11657,197...|\n",
      "|        0|[omgaga, im, sooo...|[omgaga, im, sooo...|[omgaga im, im so...|(20000,[1251,2920...|(20000,[1251,2920...|\n",
      "|        0|[i, think, mi, bf...|[think, mi, bf, c...|[think mi, mi bf,...|(20000,[3128,4257...|(20000,[3128,4257...|\n",
      "|        0|[or, i, just, wor...|       [worry, much]|        [worry much]|(20000,[13914],[1...|(20000,[13914],[8...|\n",
      "|        1|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|(20000,[13040],[1...|(20000,[13040],[8...|\n",
      "|        0|[sunny, again, wo...|[sunny, work, tom...|[sunny work, work...|(20000,[1261,6168...|(20000,[1261,6168...|\n",
      "|        1|[handed, in, my, ...|[handed, uniform,...|[handed uniform, ...|(20000,[1024,9228...|(20000,[1024,9228...|\n",
      "|        1|[hmmmm, i, wonder...|[hmmmm, wonder, n...|[hmmmm wonder, wo...|(20000,[3524,1095...|(20000,[3524,1095...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,(20000,[9509,19771],[1.0,1.0]))\n",
      "(0.0,(20000,[1429,2631,3344,3703,5070,5522,9209,17496],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n"
     ]
    }
   ],
   "source": [
    "#convert dataframe t rdd, to make a LabeledPoint tuple(label, feature, vector) for machine\n",
    "tf_rdd=tf_df.rdd\n",
    "\n",
    "from pyspark.mllib.linalg import  Vectors as MLLibVectors\n",
    "#we also need to convert ml.sparsevector mllib.sparse vector, because naive bayes only accepts mllib.sparsevector type\n",
    "train_dataset=tf_rdd.map(lambda x: LabeledPoint(float(x.sentiment), MLLibVectors.fromML(x.tf)))\n",
    "#split dataset into train, test\n",
    "train, test=train_dataset.randomSplit([0.9, 0.1], seed=11)\n",
    "print(train.first())\n",
    "print(test.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************TRAINIG*******************************\n",
      "*****************************TRAINING COMPLETE************************************\n"
     ]
    }
   ],
   "source": [
    "#create Model\n",
    "#now train and save the model\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "import shutil\n",
    "#training\n",
    "print(\"************************TRAINIG*******************************\")\n",
    "model=NaiveBayes.train(train, 1.0)\n",
    "print(\"*****************************TRAINING COMPLETE************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "\n",
    "output_dir = 'D:/HPC-proj/NaiveBayesModel_Tweet3'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model.save(sc,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************TESTING***********************************\n",
      "Model Accuracy is  0.6576188068273974\n",
      "*****************TESTING COMPLETED*****************************\n"
     ]
    }
   ],
   "source": [
    "#testing on test data\n",
    "print(\"************************TESTING***********************************\")\n",
    "predictionAndLabel=test.map(lambda x: (x.label, model.predict(x.features)))\n",
    "accuracy=1.0*predictionAndLabel.filter(lambda x: x[0]==x[1]).count()/test.count()\n",
    "print(\"Model Accuracy is \", accuracy)\n",
    "print(\"*****************TESTING COMPLETED*****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loogistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cache()\n",
    "# Run training algorithm to build the model\n",
    "model1 = LogisticRegressionWithLBFGS.train(train)\n",
    "\n",
    "#saving the model\n",
    "\n",
    "output_dir = 'D:/HPC-proj/BinaryClassifyier'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model1.save(sc,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6610248728688235\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model1.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(test.count())\n",
    "print(\"Accuracy = \" + str(1-trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.521599445380094\n",
      "Area under ROC = 0.5952628962327116\n"
     ]
    }
   ],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "#Precision-Recall curves summarize the trade-off between the true \n",
    "#positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "#ROC Curves summarize the trade-off between the true positive rate \n",
    "#and false positive rate for a predictive model using different probability thresholds.\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classification\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 5 with 35 nodes\n",
      "  If (feature 20234 <= 0.5)\n",
      "   If (feature 32678 <= 1.5)\n",
      "    If (feature 44283 <= 0.5)\n",
      "     If (feature 10064 <= 0.5)\n",
      "      If (feature 30363 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 30363 > 0.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 10064 > 0.5)\n",
      "      If (feature 25325 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 25325 > 0.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 44283 > 0.5)\n",
      "     If (feature 22986 <= 0.5)\n",
      "      If (feature 16233 <= 0.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 16233 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 22986 > 0.5)\n",
      "      If (feature 1959 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1959 > 0.5)\n",
      "       Predict: 0.0\n",
      "   Else (feature 32678 > 1.5)\n",
      "    If (feature 30372 <= 0.5)\n",
      "     If (feature 49524 <= 0.5)\n",
      "      If (feature 1310 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1310 > 0.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 49524 > 0.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 30372 > 0.5)\n",
      "     If (feature 724 <= 0.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 724 > 0.5)\n",
      "      Predict: 1.0\n",
      "  Else (feature 20234 > 0.5)\n",
      "   If (feature 13227 <= 0.5)\n",
      "    If (feature 33694 <= 2.5)\n",
      "     If (feature 2737 <= 0.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 2737 > 0.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 33694 > 2.5)\n",
      "     Predict: 1.0\n",
      "   Else (feature 13227 > 0.5)\n",
      "    If (feature 66 <= 0.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 66 > 0.5)\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = DecisionTree.trainClassifier(trained_hashed, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "#saving the model\n",
    "\n",
    "output_dir = 'D:/HPC-proj/DecisionTreeC'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model2.save(sc,output_dir)\n",
    "\n",
    "print('Learned classification tree model:')\n",
    "print(model2.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9554886211512718\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model2.predict(test_hashed.map(lambda x: x.features))\n",
    "labelsAndPredictions = test_hashed.map(lambda lp: lp.label).zip(predictions)\n",
    "testAcc = labelsAndPredictions.filter(lambda lp: lp[0] == lp[1]).count() / float(test_hashed.count())\n",
    "print('Accuracy = ' + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tress Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned regression tree model:\n",
      "DecisionTreeModel regressor of depth 5 with 35 nodes\n",
      "  If (feature 20234 <= 0.5)\n",
      "   If (feature 32678 <= 1.5)\n",
      "    If (feature 44283 <= 0.5)\n",
      "     If (feature 10064 <= 0.5)\n",
      "      If (feature 30363 <= 0.5)\n",
      "       Predict: 0.5947368421052631\n",
      "      Else (feature 30363 > 0.5)\n",
      "       Predict: 0.23096446700507614\n",
      "     Else (feature 10064 > 0.5)\n",
      "      If (feature 25325 <= 0.5)\n",
      "       Predict: 0.902317880794702\n",
      "      Else (feature 25325 > 0.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 44283 > 0.5)\n",
      "     If (feature 22986 <= 0.5)\n",
      "      If (feature 16233 <= 0.5)\n",
      "       Predict: 0.0069605568445475635\n",
      "      Else (feature 16233 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 22986 > 0.5)\n",
      "      If (feature 1959 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1959 > 0.5)\n",
      "       Predict: 0.0\n",
      "   Else (feature 32678 > 1.5)\n",
      "    If (feature 30372 <= 0.5)\n",
      "     If (feature 49524 <= 0.5)\n",
      "      If (feature 1310 <= 0.5)\n",
      "       Predict: 0.9994579945799458\n",
      "      Else (feature 1310 > 0.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 49524 > 0.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 30372 > 0.5)\n",
      "     If (feature 724 <= 0.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 724 > 0.5)\n",
      "      Predict: 1.0\n",
      "  Else (feature 20234 > 0.5)\n",
      "   If (feature 13227 <= 0.5)\n",
      "    If (feature 33694 <= 2.5)\n",
      "     If (feature 2737 <= 0.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 2737 > 0.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 33694 > 2.5)\n",
      "     Predict: 1.0\n",
      "   Else (feature 13227 > 0.5)\n",
      "    If (feature 66 <= 0.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 66 > 0.5)\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = DecisionTree.trainRegressor(trained_hashed, categoricalFeaturesInfo={},impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "#saving the model\n",
    "\n",
    "output_dir = 'D:/HPC-proj/DecisionTreeR'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model3.save(sc,output_dir)\n",
    "\n",
    "print('Learned regression tree model:')\n",
    "print(model3.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.02970745982424744\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions1 = model3.predict(test_hashed.map(lambda x: x.features))\n",
    "labelsAndPredictions1 = test_hashed.map(lambda lp: lp.label).zip(predictions1)\n",
    "testMSE = labelsAndPredictions1.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /float(test_hashed.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAndom Forest\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 43431 <= 0.5)\n",
      "     If (feature 9435 <= 0.5)\n",
      "      If (feature 17647 <= 0.5)\n",
      "       If (feature 21093 <= 0.5)\n",
      "        Predict: 0.0\n",
      "       Else (feature 21093 > 0.5)\n",
      "        Predict: 1.0\n",
      "      Else (feature 17647 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 9435 > 0.5)\n",
      "      If (feature 28997 <= 0.5)\n",
      "       If (feature 8886 <= 0.5)\n",
      "        Predict: 1.0\n",
      "       Else (feature 8886 > 0.5)\n",
      "        Predict: 0.0\n",
      "      Else (feature 28997 > 0.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 43431 > 0.5)\n",
      "     Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 39727 <= 0.5)\n",
      "     If (feature 16896 <= 3.5)\n",
      "      If (feature 22501 <= 0.5)\n",
      "       If (feature 45673 <= 0.5)\n",
      "        Predict: 0.0\n",
      "       Else (feature 45673 > 0.5)\n",
      "        Predict: 1.0\n",
      "      Else (feature 22501 > 0.5)\n",
      "       If (feature 24411 <= 0.5)\n",
      "        Predict: 1.0\n",
      "       Else (feature 24411 > 0.5)\n",
      "        Predict: 0.0\n",
      "     Else (feature 16896 > 3.5)\n",
      "      If (feature 23843 <= 0.5)\n",
      "       If (feature 39428 <= 1.5)\n",
      "        Predict: 0.0\n",
      "       Else (feature 39428 > 1.5)\n",
      "        Predict: 1.0\n",
      "      Else (feature 23843 > 0.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 39727 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 11583 <= 0.5)\n",
      "     If (feature 25991 <= 0.5)\n",
      "      If (feature 10064 <= 2.5)\n",
      "       If (feature 42054 <= 0.5)\n",
      "        Predict: 0.0\n",
      "       Else (feature 42054 > 0.5)\n",
      "        Predict: 1.0\n",
      "      Else (feature 10064 > 2.5)\n",
      "       If (feature 998 <= 0.5)\n",
      "        Predict: 1.0\n",
      "       Else (feature 998 > 0.5)\n",
      "        Predict: 0.0\n",
      "     Else (feature 25991 > 0.5)\n",
      "      If (feature 45763 <= 0.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 45763 > 0.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 11583 > 0.5)\n",
      "     If (feature 26708 <= 0.5)\n",
      "      If (feature 26929 <= 0.5)\n",
      "       If (feature 20408 <= 1.5)\n",
      "        Predict: 0.0\n",
      "       Else (feature 20408 > 1.5)\n",
      "        Predict: 1.0\n",
      "      Else (feature 26929 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 26708 > 0.5)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4 = RandomForest.trainClassifier(trained_hashed, numClasses=2, categoricalFeaturesInfo={},numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                         impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "#saving the model\n",
    "output_dir = 'D:/HPC-proj/RandomForest'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model4.save(sc,output_dir)\n",
    "\n",
    "print('Learned classification forest model:')\n",
    "print(model4.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7402945113788487\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions2 = model4.predict(test_hashed.map(lambda x: x.features))\n",
    "labelsAndPredictions2 = test_hashed.map(lambda lp: lp.label).zip(predictions2)\n",
    "testAcc1 = labelsAndPredictions2.filter(lambda lp: lp[0] == lp[1]).count() / float(test_hashed.count())\n",
    "print('Accuracy = ' + str(testAcc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = SVMWithSGD.train(train, iterations=100)\n",
    "    \n",
    "#saving the model\n",
    "output_dir = 'D:/HPC-proj/SVM'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model5.save(sc,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6369868241534924\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds1 = test.map(lambda p: (p.label, model5.predict(p.features)))\n",
    "trainErr3 = labelsAndPreds1.filter(lambda lp: lp[0] != lp[1]).count() / float(test.count())\n",
    "print(\"Accuracy = \" + str(1-trainErr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
